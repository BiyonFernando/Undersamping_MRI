{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/avishka/biyon/Undersamping_MRI/Cuda_code/IGS/notebooks/acdc', '/home/avishka/anaconda3/envs/wm/lib/python310.zip', '/home/avishka/anaconda3/envs/wm/lib/python3.10', '/home/avishka/anaconda3/envs/wm/lib/python3.10/lib-dynload', '', '/home/avishka/anaconda3/envs/wm/lib/python3.10/site-packages', '/home/avishka/biyon/Undersamping_MRI/CPU_code/IGS', '/home/avishka/biyon/Undersamping_MRI/CPU_code/IGS']\n"
     ]
    }
   ],
   "source": [
    "# %env CUDA_VISIBLE_DEVICES=2\n",
    "import sys\n",
    "# sys.path.append('/home/a_razumov/projects/k-space-mri')\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "# sys.path.append('/home/avishka/biyon/Undersamping_MRI/CPU_code/IGS' )\n",
    "# sys.path.remove('/home/avishka/biyon/Undersamping_MRI/CPU_code/IGS')\n",
    "# print(sys.path)\n",
    "\n",
    "from k_space_reconstruction.utils.metrics import pt_msssim, pt_ssim\n",
    "from k_space_reconstruction.datasets.acdc import ACDCSet, ACDCTransform, RandomMaskFunc\n",
    "from k_space_reconstruction.datasets.fastmri import FastMRIh5Dataset, FastMRITransform, LegacyFastMRIh5Dataset\n",
    "from k_space_reconstruction.utils.kspace import EquispacedMaskFunc, RandomMaskFunc\n",
    "from k_space_reconstruction.utils.kspace import pt_spatial2kspace as Ft\n",
    "from k_space_reconstruction.utils.kspace import pt_kspace2spatial as IFt\n",
    "\n",
    "import os\n",
    "from k_space_reconstruction.nets.unet import Unet\n",
    "from k_space_reconstruction.nets.enet import ENet\n",
    "from k_space_reconstruction.nets.mwcnn import MWCNN\n",
    "import datetime\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "plt.style.use('bmh')\n",
    "import albumentations\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pylab as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from os.path import isdir, join\n",
    "from typing import Callable, Dict, List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_bezzeless = lambda nc, nr : plt.subplots(ncols=nc, nrows=nr, figsize=(4 * nc, 4), dpi=120, \n",
    "                                             subplot_kw=dict(frameon=False, xticks=[], yticks=[]), \n",
    "                                             gridspec_kw=dict(wspace=0.0, hspace=0.0))\n",
    "\n",
    "\n",
    "def ce_loss(true, logits, weights, ignore=255):\n",
    "    torch.nn.CrossEntropyLoss\n",
    "    ce_loss = torch.nn.functional.cross_entropy(\n",
    "        logits.float(),\n",
    "        true.long(),\n",
    "        ignore_index=ignore,\n",
    "        weight=weights,\n",
    "    )\n",
    "    return ce_loss\n",
    "\n",
    "\n",
    "def dice_loss(true, logits, eps=1e-7):\n",
    "    num_classes = logits.shape[1]\n",
    "    if num_classes == 1:\n",
    "        true_1_hot = torch.eye(num_classes + 1)[true.squeeze(1)]\n",
    "        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
    "        true_1_hot_f = true_1_hot[:, 0:1, :, :]\n",
    "        true_1_hot_s = true_1_hot[:, 1:2, :, :]\n",
    "        true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n",
    "        pos_prob = torch.sigmoid(logits)\n",
    "        neg_prob = 1 - pos_prob\n",
    "        probas = torch.cat([pos_prob, neg_prob], dim=1)\n",
    "    else:\n",
    "        true_1_hot = torch.eye(num_classes)[true.squeeze(1)]\n",
    "        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
    "        probas = torch.nn.functional.softmax(logits, dim=1)\n",
    "    true_1_hot = true_1_hot.type(logits.type())\n",
    "    dims = (0,) + tuple(range(2, true.ndimension()))\n",
    "    intersection = torch.sum(probas * true_1_hot, dims)\n",
    "    cardinality = torch.sum(probas + true_1_hot, dims)\n",
    "    dice_loss = (2. * intersection / (cardinality + eps)).mean()\n",
    "    return (1 - dice_loss)\n",
    "\n",
    "def dice_coeffs(true, logits):\n",
    "    num_classes = logits.shape[1]\n",
    "    probas = F.softmax(logits, dim=1)\n",
    "    probas[probas > 0.5] = 1; probas[probas <= 0.5] = 0\n",
    "    pmask = torch.zeros_like(true).float()\n",
    "    for i in range(1, num_classes):\n",
    "        pmask[:,0] += i * probas[:,i]\n",
    "    dice_ls = []\n",
    "    for i in range(1, num_classes):\n",
    "        yt = (true==i).float().flatten()\n",
    "        yp = (pmask==i).float().flatten()\n",
    "        intersection = torch.sum(yt * yp)\n",
    "        cardinality = torch.sum(yt + yp)\n",
    "        dice_ls.append((2. * intersection / (cardinality + 1e-7)).item())\n",
    "    return dice_ls        \n",
    "\n",
    "def train_sampling_pattern(train_generator, model, n=14):\n",
    "    criterion = lambda p,t : dice_loss(t, p) * .75 + ce_loss(t.squeeze(1), p, weights=None) * .25\n",
    "    c, bmasks, images, bmean, bstd = next(iter(train_generator))\n",
    "    bks = Ft(images * bstd + bmean)\n",
    "    bgt = IFt(bks).abs()\n",
    "    w = torch.zeros(256, 256).cuda().float()\n",
    "    w[:,128] = 1\n",
    "    bbatch = 32\n",
    "    w_list = []\n",
    "    pbar = tqdm(range(n))\n",
    "    for count in pbar:\n",
    "        w = torch.autograd.Variable(w, requires_grad=True)\n",
    "        for j in range(bks.shape[0] // bbatch):\n",
    "            bbks = bks[bbatch*j:bbatch*(j+1)].cuda()\n",
    "            bbgt = bgt[bbatch*j:bbatch*(j+1)].cuda()\n",
    "            bbmean = bmean[bbatch*j:bbatch*(j+1)].cuda()\n",
    "            bbstd = bstd[bbatch*j:bbatch*(j+1)].cuda()\n",
    "            bbmasks = bmasks[bbatch*j:bbatch*(j+1)].cuda()\n",
    "            recs = IFt(bbks * w).abs()\n",
    "            pm = model((recs - bbmean) / (bbstd + 1e-11))\n",
    "            loss = criterion(pm, bbmasks.long())\n",
    "            loss.backward()\n",
    "        for i in torch.topk(w.grad.flatten(), 256*256, largest=False).indices:\n",
    "            if w.flatten()[i] == 0: \n",
    "                w = w.detach().flatten()\n",
    "                w[i] = 1.\n",
    "                w = w.unflatten(0, (256,256))\n",
    "                w_list.append(w.clone().cpu())\n",
    "                pbar.set_description('select: %d, loss: %.6f' % (i.item(), loss.item()))\n",
    "                break\n",
    "    return w_list\n",
    "\n",
    "def test_sampling_pattern(sampling, model, val_generator):\n",
    "    vc, vbmasks, vimages, vbmean, vbstd = next(iter(val_generator))\n",
    "    vbks = Ft(vimages * vbstd + vbmean)\n",
    "    vbgt = IFt(vbks).abs()\n",
    "    dice_scores = []\n",
    "    bbatch = 1\n",
    "    for j in tqdm(range(vbks.shape[0] // bbatch)):\n",
    "        vbbks = vbks[bbatch*j:bbatch*(j+1)].cuda()\n",
    "        vbbgt = vbgt[bbatch*j:bbatch*(j+1)].cuda()\n",
    "        vbbmean = vbmean[bbatch*j:bbatch*(j+1)].cuda()\n",
    "        vbbstd = vbstd[bbatch*j:bbatch*(j+1)].cuda()\n",
    "        vbbmasks = vbmasks[bbatch*j:bbatch*(j+1)].cuda()\n",
    "        with torch.no_grad():\n",
    "            # igs\n",
    "            recs = IFt(vbbks * sampling).abs()\n",
    "            pm = model((recs - vbbmean) / (vbbstd + 1e-11))\n",
    "            for i in range(recs.shape[0]):\n",
    "                dice_scores.append(1 - dice_loss(vbbmasks.long(), pm).item())\n",
    "    return dice_scores\n",
    "\n",
    "def test_on_classes_sampling_pattern(sampling, model, val_generator):\n",
    "    vc, vbmasks, vimages, vbmean, vbstd = next(iter(val_generator))\n",
    "    vbks = Ft(vimages * vbstd + vbmean)\n",
    "    vbgt = IFt(vbks).abs()\n",
    "    dice_scores = []\n",
    "    bbatch = 1\n",
    "    for j in tqdm(range(vbks.shape[0] // bbatch)):\n",
    "        vbbks = vbks[bbatch*j:bbatch*(j+1)].cuda()\n",
    "        vbbgt = vbgt[bbatch*j:bbatch*(j+1)].cuda()\n",
    "        vbbmean = vbmean[bbatch*j:bbatch*(j+1)].cuda()\n",
    "        vbbstd = vbstd[bbatch*j:bbatch*(j+1)].cuda()\n",
    "        vbbmasks = vbmasks[bbatch*j:bbatch*(j+1)].cuda()\n",
    "        with torch.no_grad():\n",
    "            # igs\n",
    "            recs = IFt(vbbks * sampling).abs()\n",
    "            pm = model((recs - vbbmean) / (vbbstd + 1e-11))\n",
    "            for i in range(recs.shape[0]):\n",
    "                dice_scores.append(dice_coeffs(vbbmasks.long(), pm))\n",
    "    return dice_scores\n",
    "\n",
    "class ACDCDataset(torch.utils.data.Dataset):\n",
    "    CLASSES = {0: 'NOR', 1: 'MINF', 2: 'DCM', 3: 'HCM', 4: 'RV'}\n",
    "\n",
    "    def __init__(self, hf_path: str):\n",
    "        super().__init__()\n",
    "        self.hf = h5py.File(hf_path)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.hf)\n",
    "\n",
    "    def __getitem__(self, item: int):\n",
    "        # img = self.hf[str(item)][:1]\n",
    "        img = self.hf[str(item)].get('img')\n",
    "        mask = self.hf[str(item)].get('gt')\n",
    "        c = self.hf[str(item)].attrs['class']\n",
    "        img = torch.tensor(img).float()\n",
    "        mask = torch.tensor(mask)\n",
    "        mean = img.mean().unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
    "        std = img.std().unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
    "        img = (img - mean) / (std + 1e-11)\n",
    "\n",
    "        # print('mean', mean.shape)\n",
    "        # print('std', std.shape)\n",
    "        return c, mask.unsqueeze(0), img, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_psnr(img1, img2, maxval):\n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    return 20 * torch.log10(maxval / torch.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "tensor([[[83.9895]]])\n",
      "tensor([[[86.1291]]])\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "train_dataset = ACDCDataset('/home/avishka/biyon/Dataset/Small/train25.h5')\n",
    "val_dataset = ACDCDataset('/home/avishka/biyon/Dataset/Small/val10.h5')\n",
    "c,mask,img,mean,std = train_dataset[10]\n",
    "print(img.shape)\n",
    "print(mask.shape)\n",
    "print(mean)\n",
    "print(std)\n",
    "print(len(val_dataset))\n",
    "\n",
    "train_generator = torch.utils.data.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True, num_workers=6)\n",
    "val_generator = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "# train_generator = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=6)\n",
    "# val_generator = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "model = Unet(1, 3+1).to(device).train(False).eval()\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "# model.load_state_dict(torch.load('unet-acdc-norot.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-fold validation x16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  0\n",
      "32 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32c25989ffb4ec0a89d7e1cb4d23a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 15.75 GiB total capacity; 14.83 GiB already allocated; 12.56 MiB free; 14.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     vg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(vd, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vd), shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# print(len(tg),len(vg))\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     w_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sampling_pattern\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     dice_fold_scores[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     21\u001b[0m         ours\u001b[38;5;241m=\u001b[39mtest_sampling_pattern(w_list[\u001b[38;5;241m14\u001b[39m], model, vg),\n\u001b[1;32m     22\u001b[0m         fastmri\u001b[38;5;241m=\u001b[39mtest_sampling_pattern(fastmri_mask_x16, model, vg), \n\u001b[1;32m     23\u001b[0m         center\u001b[38;5;241m=\u001b[39mtest_sampling_pattern(zm, model, vg),\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     26\u001b[0m fold_scores \u001b[38;5;241m=\u001b[39m [{k:[vv \u001b[38;5;28;01mfor\u001b[39;00m vv \u001b[38;5;129;01min\u001b[39;00m v] \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m dv\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m dv \u001b[38;5;129;01min\u001b[39;00m dice_fold_scores\u001b[38;5;241m.\u001b[39mvalues()]\n",
      "Cell \u001b[0;32mIn[43], line 68\u001b[0m, in \u001b[0;36mtrain_sampling_pattern\u001b[0;34m(train_generator, model, n)\u001b[0m\n\u001b[1;32m     66\u001b[0m w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mVariable(w, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(bks\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m bbatch):\n\u001b[0;32m---> 68\u001b[0m     bbks \u001b[38;5;241m=\u001b[39m \u001b[43mbks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbbatch\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mj\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbbatch\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     bbgt \u001b[38;5;241m=\u001b[39m bgt[bbatch\u001b[38;5;241m*\u001b[39mj:bbatch\u001b[38;5;241m*\u001b[39m(j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     70\u001b[0m     bbmean \u001b[38;5;241m=\u001b[39m bmean[bbatch\u001b[38;5;241m*\u001b[39mj:bbatch\u001b[38;5;241m*\u001b[39m(j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 15.75 GiB total capacity; 14.83 GiB already allocated; 12.56 MiB free; 14.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "fastmri_mask_x16 = torch.tensor(EquispacedMaskFunc([0.04], [16])((256, 256))[0]).float()\n",
    "zm = torch.zeros(256,256).float()\n",
    "zm[256//2 - int(16)//2 : 256//2 + int(16)//2, 256//2 - int(16)//2 : 256//2 + int(16)//2] = 1\n",
    "fm = torch.ones(256).float()\n",
    "\n",
    "dice_fold_scores = defaultdict(dict)\n",
    "\n",
    "for i, (train_id, val_id) in enumerate(KFold(shuffle=False).split(range(len(train_dataset)))):\n",
    "    print('Fold: ',i)\n",
    "    td = torch.utils.data.Subset(train_dataset, train_id)\n",
    "    vd = torch.utils.data.Subset(train_dataset, val_id)\n",
    "    print(len(td),len(vd))\n",
    "    tg = torch.utils.data.DataLoader(td, batch_size=len(td), shuffle=True)\n",
    "    vg = torch.utils.data.DataLoader(vd, batch_size=len(vd), shuffle=False)\n",
    "    # print(len(tg),len(vg))\n",
    "    w_list = train_sampling_pattern(tg, model, n=16)\n",
    "    dice_fold_scores[i] = dict(\n",
    "        ours=test_sampling_pattern(w_list[14], model, vg),\n",
    "        fastmri=test_sampling_pattern(fastmri_mask_x16, model, vg), \n",
    "        center=test_sampling_pattern(zm, model, vg),\n",
    "    )\n",
    "    \n",
    "fold_scores = [{k:[vv for vv in v] for k,v in dv.items()} for dv in dice_fold_scores.values()]\n",
    "with open('unet_fold_scores.pkl', mode='wb') as f: pickle.dump(fold_scores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unet_fold_scores.pkl', 'rb') as f: fold_scores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "df_dice = pd.DataFrame.from_dict({\n",
    "    'fastmri': [np.mean(v['fastmri']) for v in fold_scores],\n",
    "    'center': [np.mean(v['center']) for v in fold_scores],\n",
    "    'ours': [np.mean(v['ours']) for v in fold_scores]\n",
    "})\n",
    "print(scipy.stats.ttest_rel(df_dice.center, df_dice.ours))\n",
    "df_dice.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on full train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list = train_sampling_pattern(train_generator, model, n=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(w_list, 'sampling_2d_igs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list[-1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(w_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.load('sampling_2d_igs.pt')[-1]\n",
    "\n",
    "fastmri_mask_x16 = torch.tensor(EquispacedMaskFunc([0.04], [float(256*256 / w.sum())])((256, 256))[0]).cuda().float()\n",
    "zm = torch.zeros(256).cuda().float()\n",
    "zm[256//2 - int(256 / (256*256 / w.sum()))//2 : 256//2 + int(256 / (256*256 / w.sum()))//2] = 1\n",
    "zm2 = torch.zeros(256, 256).cuda().float()\n",
    "zm2[256//2 - int(w.sum()**0.5+1)//2:256//2 + int(w.sum()**0.5+1)//2, 256//2 - int(w.sum()**0.5+1)//2:256//2 + int(w.sum()**0.5+1)//2] = 1\n",
    "fm = torch.ones(256).cuda().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastmri_mask_x16.sum(), zm.sum(), zm2.sum(), w.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_scores = dict(\n",
    "    fastmri=test_sampling_pattern(fastmri_mask_x16, model, val_generator), \n",
    "    center=test_sampling_pattern(zm, model, val_generator),\n",
    "    center2d=test_sampling_pattern(zm2, model, val_generator),\n",
    "    ours=test_sampling_pattern(w.cuda(), model, val_generator),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dice = pd.DataFrame.from_dict(dice_scores)\n",
    "df_dice.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(zm2.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(w.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval dice classes scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastmri_mask_x16 = torch.tensor(EquispacedMaskFunc([0.04], [8])((256, 256))[0]).cuda().float()\n",
    "zm = torch.zeros(256).cuda().float()\n",
    "zm[256//2 - int(32)//2 : 256//2 + int(32)//2] = 1\n",
    "fm = torch.ones(256).cuda().float()\n",
    "w = torch.load('sampling_igs.pt')[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_class_scores = dict(\n",
    "    fastmri=test_on_classes_sampling_pattern(fastmri_mask_x16, model, val_generator), \n",
    "    center=test_on_classes_sampling_pattern(zm, model, val_generator),\n",
    "    ours=test_on_classes_sampling_pattern(w, model, val_generator),\n",
    "    full=test_on_classes_sampling_pattern(fm, model, val_generator),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {0: 'RV cavity', 1: 'LV myo', 2: 'LV cavity'}\n",
    "for name in dice_class_scores.keys():\n",
    "    arr = np.array(dice_class_scores[name]).T\n",
    "    print('##############', name, '##############')\n",
    "    print(pd.DataFrame.from_dict({class_map[i]:arr[i] for i in range(arr.shape[0])}).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastmri_mask_x16 = torch.tensor(EquispacedMaskFunc([0.04], [16])((256, 256))[0]).cuda().float()\n",
    "zm = torch.zeros(256).cuda().float()\n",
    "zm[256//2 - int(16)//2 : 256//2 + int(16)//2] = 1\n",
    "fm = torch.ones(256).cuda().float()\n",
    "w = torch.load('sampling_igs.pt')[14]\n",
    "\n",
    "dice_class_scores = dict(\n",
    "    fastmri=test_on_classes_sampling_pattern(fastmri_mask_x16, model, val_generator), \n",
    "    center=test_on_classes_sampling_pattern(zm, model, val_generator),\n",
    "    ours=test_on_classes_sampling_pattern(w, model, val_generator),\n",
    "    full=test_on_classes_sampling_pattern(fm, model, val_generator),\n",
    ")\n",
    "\n",
    "class_map = {0: 'RV cavity', 1: 'LV myo', 2: 'LV cavity'}\n",
    "for name in dice_class_scores.keys():\n",
    "    arr = np.array(dice_class_scores[name]).T\n",
    "    print('##############', name, '##############')\n",
    "    print(pd.DataFrame.from_dict({class_map[i]:arr[i] for i in range(arr.shape[0])}).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
